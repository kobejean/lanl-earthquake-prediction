{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7aaf8f776224015089cf92a51d99216d2aef63c5"
   },
   "source": [
    "## Import python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported!\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "from threading import Thread\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import random\n",
    "print(\"Imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "668f1f7eb0bf9f6beb3a9f2849a0d90d3ef448da"
   },
   "source": [
    "## Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "f9cf5fa9ed66cddc694419d1b572da63507abe0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Hyperparameters -----\n",
      "epochs: 37\n",
      "batch_size: 4\n",
      "feature_size: 5\n",
      "valid_steps: 4\n",
      "learning_rate: 0.0005\n",
      "---------------------------\n",
      "seg_size: 150000\n",
      "bin_size: 4096\n",
      "bins_per_seg: 37\n",
      "case_size: 151552\n",
      "target_size: 40\n",
      "granularity: 128\n",
      "seq_size: 1171\n",
      "total_cases: 4151\n",
      "total_batches: 1038\n",
      "withheld_cases: 256\n",
      "withheld_batches: 64\n",
      "valid_cases: 16\n",
      "valid_batches: 4\n",
      "train_cases: 3879\n",
      "train_batches: 970\n",
      "---------------------------\n",
      "total_size:       629145480\n",
      "withheld_size:     38797312\n",
      "train_size:       587923336\n",
      "valid_size:         2424832\n",
      "---------------------------\n",
      "withheld_percent:     6.17%\n",
      "valid_percent:        0.39%\n",
      "train_percent:       93.45%\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "total_size = 629_145_480\n",
    "seg_size = 150_000\n",
    "bin_size = 2**12 # data comes in bins of 2^12 contiguous rows (ADC with 12-bit resolution)\n",
    "bins_per_seg = (seg_size + bin_size - 1) // bin_size # ceil(seg_size / bin_size)\n",
    "case_size = bins_per_seg * bin_size\n",
    "target_size = 40\n",
    "granularity = 128\n",
    "seq_size = seg_size // granularity\n",
    "\n",
    "epochs = 1 * bins_per_seg # there are bins_per_seg number of shifts\n",
    "batch_size = 2**2\n",
    "feature_size = 5\n",
    "valid_steps = 4\n",
    "learning_rate = 0.0005\n",
    "\n",
    "total_cases = total_size // case_size\n",
    "total_batches = (total_cases + batch_size - 1) // batch_size\n",
    "\n",
    "withheld_cases = 2**8\n",
    "withheld_batches = (withheld_cases + batch_size - 1) // batch_size\n",
    "withheld_size = withheld_cases * case_size\n",
    "withheld_percent = 100 * withheld_size / total_size\n",
    "\n",
    "valid_cases = valid_steps * batch_size\n",
    "valid_batches = (valid_cases + batch_size - 1) // batch_size\n",
    "valid_size = valid_cases * case_size\n",
    "valid_percent = 100 * valid_size / total_size\n",
    "\n",
    "train_size = total_size - valid_size - withheld_size\n",
    "train_cases = total_cases - valid_cases - withheld_cases\n",
    "train_batches = (train_cases + batch_size - 1) // batch_size\n",
    "train_percent = 100 * train_size / total_size\n",
    "\n",
    "\n",
    "print(\"----- Hyperparameters -----\")\n",
    "print(\"epochs:\", epochs)\n",
    "print(\"batch_size:\", batch_size)\n",
    "print(\"feature_size:\", feature_size)\n",
    "print(\"valid_steps:\", valid_steps)\n",
    "print(\"learning_rate:\", learning_rate)\n",
    "print(\"---------------------------\")\n",
    "print(\"seg_size:\", seg_size)\n",
    "print(\"bin_size:\", bin_size)\n",
    "print(\"bins_per_seg:\", bins_per_seg)\n",
    "print(\"case_size:\", case_size)\n",
    "print(\"target_size:\", target_size)\n",
    "print(\"granularity:\", granularity)\n",
    "print(\"seq_size:\", seq_size)\n",
    "print(\"total_cases:\", total_cases)\n",
    "print(\"total_batches:\", total_batches)\n",
    "print(\"withheld_cases:\", withheld_cases)\n",
    "print(\"withheld_batches:\", withheld_batches)\n",
    "print(\"valid_cases:\", valid_cases)\n",
    "print(\"valid_batches:\", valid_batches)\n",
    "print(\"train_cases:\", train_cases)\n",
    "print(\"train_batches:\", train_batches)\n",
    "print(\"---------------------------\")\n",
    "print(\"total_size:       {0:9d}\".format(total_size))\n",
    "print(\"withheld_size:    {0:9d}\".format(withheld_size))\n",
    "print(\"train_size:       {0:9d}\".format(train_size))\n",
    "print(\"valid_size:       {0:9d}\".format(valid_size))\n",
    "print(\"---------------------------\")\n",
    "print(\"withheld_percent: {0:8.2f}%\".format(withheld_percent))\n",
    "print(\"valid_percent:    {0:8.2f}%\".format(valid_percent))\n",
    "print(\"train_percent:    {0:8.2f}%\".format(train_percent))\n",
    "print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6621873b6dc141e501087edc1d013a5a28ed6139"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "bfa2da16e1ac711eef7274cdaeed68d01a2d9028"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Done reading data!\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading data...\")\n",
    "data = pd.read_csv('../input/train.csv', dtype={'acoustic_data': np.float32, 'time_to_failure': np.float32})[:-withheld_size]\n",
    "print(\"Done reading data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7b4be734ff8a21aa6a10e3624bc56755933e3abe"
   },
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# A vector with 1's marking the end of a bin\n",
    "bin_end = np.zeros((1, bin_size // granularity), dtype=np.float32)\n",
    "bin_end[0,-1] = 1.0\n",
    "\n",
    "def create_features(df):\n",
    "    gran_mean = (df.mean(axis=1) - 4.47) / 1.5\n",
    "    gran_median = (np.median(np.abs(df), axis=1) - 4.5) / 1.5\n",
    "    gran_std = (df.std(axis=1) - 4.26) / 9.7\n",
    "    gran_max = (df.max(axis=1) - 13.83) / 19.0\n",
    "    gran_min = (df.min(axis=1) + 4.83) / 18.6\n",
    "    return np.c_[gran_mean, gran_median, gran_std, gran_max, gran_min]\n",
    "\n",
    "def create_targets(df):\n",
    "    gran_min = df.min(axis=1)\n",
    "    return np.c_[gran_min]\n",
    "\n",
    "def batch_gen(validation=False):\n",
    "    samples = np.zeros((batch_size, seq_size, feature_size))\n",
    "    targets = np.zeros((batch_size, seq_size, 1))\n",
    "    while True:\n",
    "        for shift in random.sample(range(bins_per_seg), bins_per_seg):\n",
    "            batches = valid_batches if validation else train_batches\n",
    "            cases = valid_cases if validation else train_cases\n",
    "            case_idxs = random.sample(range(cases), cases)\n",
    "            for batch in range(batches):\n",
    "                start_case = batch * batch_size\n",
    "                end_case = start_case + batch_size\n",
    "\n",
    "                for i, case in enumerate(case_idxs[start_case:end_case]):\n",
    "                    start_row = case * case_size + shift * bin_size\n",
    "                    end_row = start_row + seg_size\n",
    "                    seg = data[start_row:end_row]\n",
    "                    acoustic = seg.acoustic_data.values\n",
    "                    time_fail = seg.time_to_failure.values\n",
    "                    full_bins = acoustic[:granularity*(seq_size-1)]\n",
    "                    full_bins = full_bins.reshape(seq_size-1, -1)\n",
    "                    unfull_bins = np.expand_dims(acoustic[granularity*(seq_size-1):],axis=0)\n",
    "                    full_bins_time = time_fail[:granularity*(seq_size-1)]\n",
    "                    full_bins_time = full_bins_time.reshape(seq_size-1, -1)\n",
    "                    unfull_bins_time = np.expand_dims(time_fail[granularity*(seq_size-1):],axis=0)\n",
    "                    samples[i,:seq_size-1] = create_features(full_bins)\n",
    "                    samples[i,-1] = create_features(unfull_bins)\n",
    "                    targets[i,:seq_size-1] = create_targets(full_bins_time)\n",
    "                    targets[i,-1] = create_targets(unfull_bins_time)\n",
    "#                     targets[i] = seg.tail(1).time_to_failure.values\n",
    "                yield samples, targets\n",
    "\n",
    "train = batch_gen()\n",
    "valid = batch_gen(validation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d17d1cd0e8fadbd9c8bc0fcc99c01f5847853ad9"
   },
   "source": [
    "## Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "c1634fd396b446e728d59c2af66490317173d771"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.22709981]\n",
      "  [0.22709967]\n",
      "  [0.22709952]\n",
      "  ...\n",
      "  [0.18879755]\n",
      "  [0.18879741]\n",
      "  [0.18879715]]\n",
      "\n",
      " [[5.36969995]\n",
      "  [5.36969948]\n",
      "  [5.36969948]\n",
      "  ...\n",
      "  [5.33139753]\n",
      "  [5.33139753]\n",
      "  [5.33139706]]\n",
      "\n",
      " [[4.51819992]\n",
      "  [4.51819944]\n",
      "  [4.51819944]\n",
      "  ...\n",
      "  [4.4798975 ]\n",
      "  [4.4798975 ]\n",
      "  [4.47989702]]\n",
      "\n",
      " [[1.03059983]\n",
      "  [1.03059971]\n",
      "  [1.03059959]\n",
      "  ...\n",
      "  [0.99229759]\n",
      "  [0.99229747]\n",
      "  [0.99229717]]]\n"
     ]
    }
   ],
   "source": [
    "print(next(train)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a2192a9ef4352a836c154330611990bfad693f92"
   },
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "76c0b1555461c08b2f68cde4566455eafe2fde00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_13 (Conv1D)           (None, 1171, 64)          704       \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 1171, 64)          8256      \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 1171, 64)          8256      \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 1171, 64)          8256      \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 1171, 64)          8256      \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 1171, 64)          8256      \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1171, 64)          8256      \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 1171, 64)          8256      \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 1171, 128)         16512     \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 1171, 128)         32896     \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 1171, 128)         49280     \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 1171, 128)         49280     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1171, 128)         16512     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1171, 128)         0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1171, 1)           129       \n",
      "=================================================================\n",
      "Total params: 223,105\n",
      "Trainable params: 223,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/37\n",
      "970/970 [==============================] - 364s 375ms/step - loss: 2.6520 - val_loss: 2.2654\n",
      "Epoch 2/37\n",
      "467/970 [=============>................] - ETA: 3:08 - loss: 2.4183"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.optimizers import adam\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from time import time\n",
    "\n",
    "cb = [ModelCheckpoint(\"dcnn-model.hdf5\", monitor='val_loss', save_weights_only=False, period=3)]\n",
    "if True:\n",
    "    cb += [TensorBoard(log_dir=\"logs/{}\".format(time()))]\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Conv1D(32, 2, activation='relu', dilation_rate=2**0, strides=1, input_shape=(seq_size, feature_size)))\n",
    "# model.add(Conv1D(32, 2, activation='relu', dilation_rate=2**1, strides=1, input_shape=(None, 32)))  \n",
    "# model.add(Conv1D(64, 2, activation='relu', dilation_rate=2**2, strides=1, input_shape=(None, 64))) \n",
    "# model.add(Conv1D(128, 2, activation='relu', dilation_rate=2**3, strides=1, input_shape=(None, 64))) \n",
    "# model.add(Conv1D(128, 2, activation='relu', dilation_rate=2**4, strides=1, input_shape=(None, 128))) \n",
    "# model.add(Conv1D(256, 2, activation='relu', dilation_rate=2**5, strides=1, input_shape=(None, 128))) \n",
    "# model.add(Conv1D(256, 2, activation='relu', dilation_rate=2**6, strides=1, input_shape=(None, 256)))\n",
    "# model.add(Conv1D(256, 3, activation='relu', dilation_rate=2**7, strides=1, input_shape=(None, 256))) \n",
    "# model.add(Conv1D(256, 3, activation='relu', dilation_rate=2**7*3, strides=1, input_shape=(None, 256))) \n",
    "# # model.add(MaxPooling1D(44, strides=16, padding='same'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(GlobalAveragePooling1D())\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(1, activation='linear'))\n",
    "model = Sequential()\n",
    "model.add(Conv1D(64, 2, activation='relu', dilation_rate=2**0, strides=1, input_shape=(seq_size, feature_size), padding='causal'))\n",
    "model.add(Conv1D(64, 2, activation='relu', dilation_rate=2**1, strides=1, input_shape=(None, 64), padding='causal')) \n",
    "model.add(Conv1D(64, 2, activation='relu', dilation_rate=2**2, strides=1, input_shape=(None, 64), padding='causal')) \n",
    "model.add(Conv1D(64, 2, activation='relu', dilation_rate=2**3, strides=1, input_shape=(None, 64), padding='causal'))  \n",
    "model.add(Conv1D(64, 2, activation='relu', dilation_rate=2**0, strides=1, input_shape=(None, 64), padding='causal'))  \n",
    "model.add(Conv1D(64, 2, activation='relu', dilation_rate=2**1, strides=1, input_shape=(None, 64), padding='causal')) \n",
    "model.add(Conv1D(64, 2, activation='relu', dilation_rate=2**2, strides=1, input_shape=(None, 64), padding='causal')) \n",
    "model.add(Conv1D(64, 2, activation='relu', dilation_rate=2**3, strides=1, input_shape=(None, 64), padding='causal')) \n",
    "model.add(Conv1D(128, 2, activation='relu', dilation_rate=2**4, strides=1, input_shape=(None, 64), padding='causal')) \n",
    "model.add(Conv1D(128, 2, activation='relu', dilation_rate=2**5, strides=1, input_shape=(None, 128), padding='causal')) \n",
    "model.add(Conv1D(128, 3, activation='relu', dilation_rate=2**6, strides=1, input_shape=(None, 128), padding='causal')) \n",
    "model.add(Conv1D(128, 3, activation='relu', dilation_rate=2**6*3, strides=1, input_shape=(None, 128), padding='causal')) \n",
    "# model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile and fit model\n",
    "model.compile(optimizer=adam(lr=learning_rate), loss=\"mae\")\n",
    "\n",
    "history = model.fit_generator(train,\n",
    "                              steps_per_epoch=train_batches,\n",
    "                              epochs=epochs,\n",
    "                              verbose=1,\n",
    "                              callbacks=cb,\n",
    "                              validation_data=valid,\n",
    "                              validation_steps=valid_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1a8c972d212557beb2f7550c02e62663f872a8ce"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Visualize accuracies\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def perf_plot(history, what = 'loss'):\n",
    "    x = history.history[what]\n",
    "    val_x = history.history['val_' + what]\n",
    "    epochs = np.asarray(history.epoch) + 1\n",
    "\n",
    "    plt.plot(epochs, x, 'bo', label = \"Training \" + what)\n",
    "    plt.plot(epochs, val_x, 'b', label = \"Validation \" + what)\n",
    "    plt.title(\"Training and validation \" + what)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "perf_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3b6ad90328ba3962ede6d26ec5018c72f734f827"
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id', dtype={\"time_to_failure\": np.float32})\n",
    "\n",
    "# Load each test data, create the feature matrix, get numeric prediction\n",
    "for i, seg_id in enumerate(submission.index):\n",
    "    seg = pd.read_csv('../input/test/' + seg_id + '.csv', dtype={'acoustic_data': np.float32})\n",
    "    x = create_features(seg)\n",
    "    predict = model.predict(np.expand_dims(x, 0))\n",
    "    print('\\r', i, seg_id, submission.shape[0], predict, end = '')\n",
    "    submission.time_to_failure[i] = predict\n",
    "\n",
    "submission.head()\n",
    "\n",
    "# Save\n",
    "submission.to_csv('submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1024b06fd1c95b1ad39c943fd9427871c0bda4fe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
