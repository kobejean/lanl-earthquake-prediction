{
  "cells": [
    {
      "metadata": {
        "_uuid": "fdfcf96bb48102a696d32138d8d8a1a40f1ac09c"
      },
      "cell_type": "markdown",
      "source": "# Prototype 1"
    },
    {
      "metadata": {
        "_uuid": "655de847e1ea99083567fbefa1474c1bd833a96f"
      },
      "cell_type": "markdown",
      "source": "## Load Data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5d64140125989893ceb6f413472faeb695258450",
        "_kg_hide-input": true,
        "_kg_hide-output": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport sys",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(os.listdir(\"../input\"))\n\nprint(\"Reading data...\")\n# Any results you write to the current directory are saved as output.\nchunksize = 2**21\ndataset = pd.read_csv(\"../input/train.csv\", dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64}, chunksize=chunksize)\n\ndef update_progress(progress, title=\"\"):\n    print('\\r{0} [{1}] {2}%'.format(title, '#'*int(progress/2)+' '*int(50-progress/2), progress), end='')\n\n\ndfs = []\nloaded_bytes = 0.0\nfor i, chunck in enumerate(dataset):\n    for j, g in chunck.groupby((chunck.time_to_failure.diff() > 0).cumsum()):\n        frags = []\n        for k, h in g.groupby((g.time_to_failure.diff() < -0.0001).cumsum()):\n            frags.append(h)\n            loaded_bytes += float(h.memory_usage(index=True, deep=True).sum())\n        if j == 0 and len(dfs) > 0 and dfs[-1][-1].tail(1).time_to_failure.values[0] >= g.head(1).time_to_failure.values[0]:\n            dfs[-1] += frags\n        else:\n            dfs.append(frags)\n    update_progress(int(100.0 * loaded_bytes / 11324618640.0), \"Earthquake: {0:3d} Fragment: {1:8d} \".format(len(dfs), len(dfs[-1])))",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['test', 'train.csv', 'sample_submission.csv']\nReading data...\nEarthquake:  17 Fragment:     1751  [##################################################] 100%",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "838fed33e8dba4d241eb3f57aad710a82532b3d8"
      },
      "cell_type": "code",
      "source": "for frag in dfs:\n    for df in frag:\n        print(df.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "50c2fcc723676f04de62f4538bb910fe3a561f1b"
      },
      "cell_type": "markdown",
      "source": "# Data Visualization"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\n# df_concat = pd.concat(dfs[1])\n# df_concat.plot(kind='line',x='time_to_failure',y='acoustic_data', figsize=(100,20))\n\ndfs[1].plot(kind='line',x='time_to_failure',y='acoustic_data', figsize=(100,20))\n\nprint(dfs[1][-1].shape[0], len(dfs[0]), len(dfs[1]))",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f9081d67a92190e4c0ab9fc8ff138cd74a25f62c"
      },
      "cell_type": "code",
      "source": "def create_features(frag_id, frag, X):\n    xc = frag['acoustic_data'].values\n    zc = np.fft.fft(xc)\n    xc_abs = np.abs(xc)\n        \n    X.loc[frag_id, 'mean'] = xc.mean()\n    X.loc[frag_id, 'abs_mean'] = xc_abs.mean()\n    X.loc[frag_id, 'std'] = xc.std()\n    X.loc[frag_id, 'var'] = xc.var()\n    X.loc[frag_id, 'max'] = xc.max()\n    X.loc[frag_id, 'min'] = xc.min()\n    X.loc[frag_id, 'abs_max'] = xc_abs.max()\n    \n    X.loc[frag_id, 'A0'] = abs(zc[0])\n    X.loc[frag_id, 'A1'] = abs(zc[1])\n    X.loc[frag_id, 'A2'] = abs(zc[2])\n    X.loc[frag_id, 'A3'] = abs(zc[3])\n    X.loc[frag_id, 'A4'] = abs(zc[4])\n    X.loc[frag_id, 'A5'] = abs(zc[5])\n    X.loc[frag_id, 'A6'] = abs(zc[6])\n    X.loc[frag_id, 'A7'] = abs(zc[7])\n    X.loc[frag_id, 'A8'] = abs(zc[8])\n    X.loc[frag_id, 'A9'] = abs(zc[9])\n    X.loc[frag_id, 'A10'] = abs(zc[10])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e75b50af5c91909300933ac128b3b0a165a7601a"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a2c99b3eb7ec32a3747c76cfeabe83b789699141"
      },
      "cell_type": "code",
      "source": "last_time = []\nstats_25 = {\n    \"mean\": [],\n    \"std\": [],\n    \"min\": [],\n    \"max\": []\n}\nstats_rolling = {\n    \"mean\": [],\n    \"std\": [],\n    \"min\": [],\n    \"max\": []\n}\nn = 0\nfor frag in tqdm(dfs):\n    for i, df in enumerate(frag):\n        abs_acoustic_data_data = df.acoustic_data.abs()\n        stats_25[\"mean\"].append(abs_acoustic_data_data.mean())\n        stats_25[\"std\"].append(df.acoustic_data.std())\n        stats_25[\"min\"].append(abs_acoustic_data_data.min())\n        stats_25[\"max\"].append(abs_acoustic_data_data.max())\n        if n % 25 == 24 or i == len(frag)-1:\n            stats_rolling[\"mean\"].append(sum(stats_25[\"mean\"])/len(stats_25[\"mean\"]))\n            stats_rolling[\"std\"].append(sum(stats_25[\"std\"])/len(stats_25[\"std\"]))\n            stats_rolling[\"min\"].append(sum(stats_25[\"min\"])/len(stats_25[\"min\"]))\n            stats_rolling[\"max\"].append(sum(stats_25[\"max\"])/len(stats_25[\"max\"]))\n            last_time.append(df.tail(1).time_to_failure.values[0])\n            stats_25[\"mean\"] = []\n            stats_25[\"std\"] = []\n            stats_25[\"min\"] = []\n            stats_25[\"max\"] = []\n        n += 1\nrolling_mean = np.array(stats_rolling[\"mean\"])\nrolling_std = np.array(stats_rolling[\"std\"])\nrolling_min = np.array(stats_rolling[\"min\"])\nrolling_max = np.array(stats_rolling[\"max\"])\nlast_time = np.array(last_time)\nprint(\"Done!\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3a304f81be33b152ad73fa3b477383e56eab21bb"
      },
      "cell_type": "code",
      "source": "\n# pd.set_option(\"display.precision\", 15)\n# plot rolling mean\nfig, ax1 = plt.subplots(figsize=(20, 10))\nfig.suptitle('Mean for chunks of 150,000 samples of training data', fontsize=14)\n\nax2 = ax1.twinx()\nax3 = ax2.twinx()\nax4 = ax2.twinx()\nax1.set_xlabel('index')\nax1.set_ylabel('Acoustic data')\nax2.set_ylabel('Time to failure')\nax3.set_ylabel('Acoustic data std')\nax4.set_ylabel('Acoustic data max')\n\nwindow_size = 100\nwindow_num = 0\nstart = window_num*window_size\nend = (window_num+1)*window_size\nstart = 0\nend = -1\nthreshold_filter = rolling_mean < 8\np1 = sns.lineplot(data=rolling_mean[threshold_filter][start:end], ax=ax1, color='orange')\np2 = sns.lineplot(data=last_time[threshold_filter][start:end], ax=ax2, color='gray')\np3 = sns.lineplot(data=rolling_std[threshold_filter][start:end], ax=ax3, color='green')\np5 = sns.lineplot(data=rolling_max[threshold_filter][start:end], ax=ax4, color='blue')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1e9829488d7981bf55132d9a3c32dfb1301e2033"
      },
      "cell_type": "code",
      "source": "os.listdir(\"../input/test/\")\ntest_file = \"../input/test/\" + os.listdir(\"../input/test/\")[0]\nprint(test_file)\ndataset_test = pd.read_csv(test_file)\nfor i in range(10):\n    shift = 2**14 + i * 2**14\n    window = 2**15\n    start = shift - window//2\n    end = shift + window//2\n    dataset_test[start:end].plot(kind='line',y='acoustic_data', figsize=(400,5))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1d59453ee04dc9ef08bd617d43dff6f52a310f6c"
      },
      "cell_type": "markdown",
      "source": "## Fragment Stats\n```\nprint(count, frag_duration, frag_gap_ave, frag_gap_ave/diff_ave, diff_ave)\n\n4096 -4.504500000335199e-06 -0.026599999999999957 24181818.18169926 -1.1000000000054078e-09\ncount  frag_duration  frag_gap_ave frag_gap_ave/diff_ave diff_ave\n4096   -4.5045e-06    -0.0266      24181818.181818       -1.1e-09\n\n\nprint(frag_gap_ave, frag_duration_ave, 25*frag_gap_ave/frag_duration_ave, diff_ave_cum/n)\n-0.02655007691140222 -4.504480206630345e-06 147353.72170312778 -1.0999924151498534e-09\n```"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0c44dfce5e30df25d06986c564a3e9f128e377f7",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "last_ave_1 = 0\nlast_ave_2 = 0\nlast_ave_3 = 0\nlast_ave_4 = 0\nlast_ave_5 = 0\nlast_ave_6 = 0\nlast_ave_7 = 0\nlast_ave_8 = 0\nlast_ave_9 = 0\nlast_ave_10 = 0\nlast_ave_11 = 0\nlast_ave_12 = 0\nlast_ave_13 = 0\nlast_ave_14 = 0\nlast_ave_15 = 0\nlast_ave_16 = 0\nlast_ave_17 = 0\nlast_ave_18 = 0\nlast_ave_19 = 0\nlast_ave_20 = 0\nlast_ave_21 = 0\nlast_ave_22 = 0\nlast_ave_23 = 0\nlast_ave_24 = 0\nlast_ave_25 = 0\n\n\nn = 0\nfrag_duration_cum = 0\nfrag_gap_cum = 0\ndiff_ave_cum = 0\nfor frag in dfs:\n    for df in frag:\n        count = len(df.index)\n        head = df.head(1).time_to_failure.values[0]\n        tail = df.tail(1).time_to_failure.values[0]\n        ave = df.time_to_failure.mean()\n        diff_ave = df.tail(4095).time_to_failure.diff().mean()\n        frag_duration = tail-head\n        frag_gap_ave = ave-last_ave_25\n        if frag_gap_ave < 0 and count == 4096:\n            n += 1\n            frag_gap_cum += frag_gap_ave\n            frag_duration_cum += frag_duration\n            diff_ave_cum += diff_ave\n        print(count, frag_duration, frag_gap_ave, frag_gap_ave/diff_ave, diff_ave)\n        last_ave_25 = last_ave_24\n        last_ave_24 = last_ave_23\n        last_ave_23 = last_ave_22\n        last_ave_22 = last_ave_21\n        last_ave_21 = last_ave_20\n        last_ave_20 = last_ave_19\n        last_ave_19 = last_ave_18\n        last_ave_18 = last_ave_17\n        last_ave_17 = last_ave_16\n        last_ave_16 = last_ave_15\n        last_ave_15 = last_ave_14\n        last_ave_14 = last_ave_13\n        last_ave_13 = last_ave_12\n        last_ave_12 = last_ave_11\n        last_ave_11 = last_ave_10\n        last_ave_10 = last_ave_9\n        last_ave_9 = last_ave_8\n        last_ave_8 = last_ave_7\n        last_ave_7 = last_ave_6\n        last_ave_6 = last_ave_5\n        last_ave_5 = last_ave_4\n        last_ave_4 = last_ave_3\n        last_ave_3 = last_ave_2\n        last_ave_2 = last_ave_1\n        last_ave_1 = ave\nfrag_gap_ave = frag_gap_cum/n\nfrag_duration_ave = frag_duration_cum/n\nprint(frag_gap_ave, frag_duration_ave, 25*frag_gap_ave/frag_duration_ave, diff_ave_cum/n)\n# 967,491.030451251 = 4096*236.20386485626256\n# 967,489.195553758 = 4096*236.2034168832417\n# 967,491.030447658 = 4096*236.20386485538532\n# 1,000,133.18541318 = \n#",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}