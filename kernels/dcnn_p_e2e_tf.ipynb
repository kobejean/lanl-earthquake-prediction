{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dcnn-p-e2e-tf.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "_qI80FIWDoYf",
        "colab_type": "code",
        "outputId": "a8656124-ba23-43fb-faba-e80668d1e0ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 975
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "!python3 -m pip install dhooks"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-02-07 06:32:58--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 34.206.9.96, 34.238.3.58, 34.231.75.48, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|34.206.9.96|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5363700 (5.1M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]   5.11M  15.0MB/s    in 0.3s    \n",
            "\n",
            "2019-02-07 06:32:59 (15.0 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [5363700/5363700]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n",
            "Collecting dhooks\n",
            "  Downloading https://files.pythonhosted.org/packages/da/59/b337b07dd6ef8dd9af53def6f67283809a5d0292863c229045aaf06c4c56/dhooks-1.1.0.tar.gz\n",
            "Collecting aiohttp (from dhooks)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0d/5c/f87987f4dc8b2cfcf37c83a814ea4b2aff4d285cbffc0ab08b2b4fa3f584/aiohttp-3.5.4-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.2MB 15.0MB/s \n",
            "\u001b[?25hCollecting requests==2.20.1 (from dhooks)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/17/5cbb026005115301a8fb2f9b0e3e8d32313142fe8b617070e7baad20554f/requests-2.20.1-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 16.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->dhooks) (3.0.4)\n",
            "Collecting idna-ssl>=1.0; python_version < \"3.7\" (from aiohttp->dhooks)\n",
            "  Downloading https://files.pythonhosted.org/packages/46/03/07c4894aae38b0de52b52586b24bf189bb83e4ddabfe2e2c8f2419eec6f4/idna-ssl-1.1.0.tar.gz\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->dhooks)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/c0/9a73968a9f4e4dac8dffb0ba35f932dd7798fe97901f4942c2d38667862c/yarl-1.3.0-cp36-cp36m-manylinux1_x86_64.whl (251kB)\n",
            "\u001b[K    100% |████████████████████████████████| 256kB 28.3MB/s \n",
            "\u001b[?25hCollecting multidict<5.0,>=4.0 (from aiohttp->dhooks)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/cc/ceb5b8c76e7a23212b9e0353053cc35a9d86c763d852a76d9b941fe81fbc/multidict-4.5.2-cp36-cp36m-manylinux1_x86_64.whl (309kB)\n",
            "\u001b[K    100% |████████████████████████████████| 317kB 29.5MB/s \n",
            "\u001b[?25hCollecting typing-extensions>=3.6.5; python_version < \"3.7\" (from aiohttp->dhooks)\n",
            "  Downloading https://files.pythonhosted.org/packages/0f/62/c66e553258c37c33f9939abb2dd8d2481803d860ff68e635466f12aa7efa/typing_extensions-3.7.2-py3-none-any.whl\n",
            "Collecting async-timeout<4.0,>=3.0 (from aiohttp->dhooks)\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->dhooks) (18.2.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.20.1->dhooks) (1.22)\n",
            "Requirement already satisfied: idna<2.8,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.20.1->dhooks) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.20.1->dhooks) (2018.11.29)\n",
            "Building wheels for collected packages: dhooks, idna-ssl\n",
            "  Building wheel for dhooks (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/06/96/0e/abb2ead42895b4736064f77c5e829e4dfff6e280e0add6a39c\n",
            "  Building wheel for idna-ssl (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/d3/00/b3/32d613e19e08a739751dd6bf998cfed277728f8b2127ad4eb7\n",
            "Successfully built dhooks idna-ssl\n",
            "\u001b[31mspacy 2.0.18 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mgoogle-colab 0.0.1a1 has requirement requests~=2.18.0, but you'll have requests 2.20.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mcufflinks 0.14.6 has requirement plotly>=3.0.0, but you'll have plotly 1.12.12 which is incompatible.\u001b[0m\n",
            "Installing collected packages: idna-ssl, multidict, yarl, typing-extensions, async-timeout, aiohttp, requests, dhooks\n",
            "  Found existing installation: requests 2.18.4\n",
            "    Uninstalling requests-2.18.4:\n",
            "      Successfully uninstalled requests-2.18.4\n",
            "Successfully installed aiohttp-3.5.4 async-timeout-3.0.1 dhooks-1.1.0 idna-ssl-1.1.0 multidict-4.5.2 requests-2.20.1 typing-extensions-3.7.2 yarl-1.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qJBLkHiXWB4H",
        "colab_type": "code",
        "outputId": "454e2ec1-9877-41f7-f77b-ded764781286",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in\n",
        "# JAKE COMMENT!!!!!!!!!!!!!!!!!!!!!!\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from dhooks import Webhook, Embed\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import random\n",
        "import time\n",
        "\n",
        "print(\"Imported!\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Imported!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OBYOdVFKDt0P",
        "colab_type": "code",
        "outputId": "c793f272-cd40-4478-f6ed-6faa9abb2b40",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "cell_type": "code",
      "source": [
        "model_name = 'dcnn-p-e2e-tf'\n",
        "discord_webhook_url = 'https://discordapp.com/api/webhooks/541724267461476388/6RSn5xYXbr-bPy-nV1P4FuSLQJLQXx31xG76gCqva3UVnA2eavbKi4boZdKPHE24jjQq'  #@param {type:\"string\"}\n",
        "total_size = 629_145_480\n",
        "seg_size = 150_000\n",
        "packet_size = 2**12 # data comes in bins of 2^12 contiguous rows (ADC with 12-bit resolution)\n",
        "\n",
        "epochs = 1600 #16 * bins_per_seg\n",
        "train_batch_size = 128 #@param {type:\"integer\"}\n",
        "valid_batch_size = 128 #@param {type:\"integer\"}\n",
        "test_batch_size = 128 #@param {type:\"integer\"}\n",
        "\n",
        "total_earthquakes = 16\n",
        "valid_earthquakes = 2  #@param {type:\"integer\"}\n",
        "test_earthquakes = 2  #@param {type:\"integer\"}\n",
        "train_earthquakes = total_earthquakes - valid_earthquakes - test_earthquakes\n",
        "\n",
        "packets = [1381-2, 10847-8, 13328-10, 8324-7, 11931-9, 7571-6, 6635-5, 15139-12, 7431-6, 9058-7, 10740-8, 10362-9, 8298-6, 8051-6, 13865-11, 8891-7]#, 1748-2]\n",
        "\n",
        "\n",
        "# learning_rate = 0.00065 #@param {type:\"number\"}\n",
        "# dilation_rates = [1, 4, 16, 64, 256, 1024] * 3\n",
        "# kernel_sizes = [4] * 6 * 3\n",
        "# filter_width = 32 #@param {type:\"number\"}\n",
        "\n",
        "# random_seed = 42 #@param {type:\"integer\"}\n",
        "\n",
        "experiments = [\n",
        "    {\n",
        "        'experiment_name': 'cv-x4',\n",
        "        'description': 'cross validating experiment 4 - shuffle all packets but without reshuffling every epoch',\n",
        "        'learning_rate': 0.00125,\n",
        "        'dilation_rates': [1, 4, 16, 64, 256, 1024] ,\n",
        "        'kernel_sizes': [4] * 6,\n",
        "        'filter_width': 32,\n",
        "        'random_seed': 42,\n",
        "        'max_epochs': 120,\n",
        "        'epochs_per_validation': 30,\n",
        "        'early_stopping_epsilon': 0.5,\n",
        "        'use_last_mae_loss': False,\n",
        "        'padding': 'causal',\n",
        "        'cross_validation_sets': [2,3,4,5,6,0,1],\n",
        "        'reshuffle_training_data': True\n",
        "    },\n",
        "#     {\n",
        "#         'experiment_name': 'x4-non-causal',\n",
        "#         'description': 'cross validating experiment 2 - every batch has all training earthquakes',\n",
        "#         'learning_rate': 0.00065,\n",
        "#         'dilation_rates': [1, 4, 16, 64, 256, 1024],\n",
        "#         'kernel_sizes': [4] * 6,\n",
        "#         'filter_width': 32,\n",
        "#         'random_seed': 42,\n",
        "#         'max_epochs': 100,\n",
        "#         'epochs_per_validation': 30,\n",
        "#         'early_stopping_epsilon': 0.8,\n",
        "#         'use_last_mae_loss': False,\n",
        "#         'padding': 'same',\n",
        "#         'cross_validation_sets': [0,1],\n",
        "#         'reshuffle_training_data': False\n",
        "#     },\n",
        "]\n",
        "\n",
        "### Google Cloud Storage Auth ###\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import re\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "use_tpu = True #@param {type:\"boolean\"}\n",
        "bucket = 'lanl-earthquake-prediction' #@param {type:\"string\"}\n",
        "\n",
        "assert bucket, 'Must specify an existing GCS bucket name'\n",
        "print('Using bucket: {}'.format(bucket))\n",
        "\n",
        "if use_tpu:\n",
        "    assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n",
        "\n",
        "DATASET_PATH = 'gs://{}/{}'.format(bucket, 'datasets/')\n",
        "print('Using dataset path: {}'.format(DATASET_PATH))\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  TF_MASTER = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
        "  \n",
        "  # Upload credentials to TPU.\n",
        "  with tf.Session(TF_MASTER) as sess:    \n",
        "    with open('/content/adc.json', 'r') as f:\n",
        "      auth_info = json.load(f)\n",
        "    tf.contrib.cloud.configure_gcs(sess, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU.\n",
        "else:\n",
        "  TF_MASTER=''\n",
        "\n",
        "with tf.Session(TF_MASTER) as session:\n",
        "  pprint.pprint(session.list_devices())\n",
        "  \n",
        "### Google Cloud Storage Auth End ###\n",
        "\n",
        "def experimentsSummary(params):\n",
        "  lines = []\n",
        "  lines.append(\"Experiment\")\n",
        "  lines.append(\"---------------------------\")\n",
        "  lines.append(\"\")\n",
        "  lines.append(\"key | value\")\n",
        "  lines.append(\"------------ | ------------\")\n",
        "  lines.append(\"experiment_name | \" + str(params['experiment_name']))\n",
        "  lines.append(\"description | \" + str(params['description']))\n",
        "  lines.append(\"learning_rate | \" + str(params['learning_rate']))\n",
        "  lines.append(\"dilation_rates | \" + str(params['dilation_rates']))\n",
        "  lines.append(\"kernel_sizes | \" + str(params['kernel_sizes']))\n",
        "  lines.append(\"filter_width | \" + str(params['filter_width']))\n",
        "  lines.append(\"random_seed | \" + str(params['random_seed']))\n",
        "  lines.append(\"max_epochs | \" + str(params['max_epochs']))\n",
        "  lines.append(\"epochs_per_validation | \" + str(params['epochs_per_validation']))\n",
        "  lines.append(\"early_stopping_epsilon | \" + str(params['early_stopping_epsilon']))\n",
        "  lines.append(\"use_last_mae_loss | \" + str(params['use_last_mae_loss']))\n",
        "  lines.append(\"padding | \" + str(params['padding']))\n",
        "  lines.append(\"cross_validation_sets | \" + str(params['cross_validation_sets']))\n",
        "  lines.append(\"reshuffle_training_data | \" + str(params['reshuffle_training_data']))\n",
        "#   lines.append(\"---------------------------\")\n",
        "  return '\\n'.join(str(x) for x in lines) \n",
        "\n",
        "for params in experiments:\n",
        "  print(experimentsSummary(params))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using bucket: lanl-earthquake-prediction\n",
            "Using dataset path: gs://lanl-earthquake-prediction/datasets/\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 16832764385906886893),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 11884467485365774112),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 4831294019828008607),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 2500098388126251820),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 10128368460189152667),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 5068711343869401661),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 42833022162495869),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 6046325417978066240),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 17868592339643487845),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 5839991267622860419),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 12107746342976545239)]\n",
            "Experiment\n",
            "---------------------------\n",
            "\n",
            "key | value\n",
            "------------ | ------------\n",
            "experiment_name | cv-x4\n",
            "description | cross validating experiment 4 - shuffle all packets but without reshuffling every epoch\n",
            "learning_rate | 0.00125\n",
            "dilation_rates | [1, 4, 16, 64, 256, 1024]\n",
            "kernel_sizes | [4, 4, 4, 4, 4, 4]\n",
            "filter_width | 32\n",
            "random_seed | 42\n",
            "max_epochs | 120\n",
            "epochs_per_validation | 30\n",
            "early_stopping_epsilon | 0.5\n",
            "use_last_mae_loss | False\n",
            "padding | causal\n",
            "cross_validation_sets | [2, 3, 4, 5, 6, 0, 1]\n",
            "reshuffle_training_data | True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aS1NXzNPgriF",
        "colab_type": "code",
        "outputId": "be7f4499-70ed-4496-9436-e55fc258b7de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.data import TFRecordDataset, Dataset\n",
        "\n",
        "def parse_function(example_proto):\n",
        "    default_value = [0] * packet_size\n",
        "    data = {\"acoustic_data\": tf.FixedLenFeature((packet_size, 1), tf.float32, default_value=default_value),\n",
        "            \"time_to_failure\": tf.FixedLenFeature((packet_size), tf.float32, default_value=default_value)}\n",
        "    parsed_data = tf.parse_single_example(example_proto, data)\n",
        "    acoustic_data = parsed_data[\"acoustic_data\"]\n",
        "    time_to_failure = parsed_data[\"time_to_failure\"]\n",
        "    return acoustic_data, time_to_failure\n",
        "\n",
        "\n",
        "def train_fn(params):\n",
        "    print(params)\n",
        "    batch_size = params['batch_size']\n",
        "    train_batches = params['train_batches'] \n",
        "    train_tf_record_files = params['train_tf_record_files']\n",
        "    random_seed = params['random_seed']\n",
        "    reshuffle_training_data = params['reshuffle_training_data']\n",
        "    files = Dataset.from_tensor_slices(train_tf_record_files)\n",
        "    dataset = (TFRecordDataset(files, num_parallel_reads=4)\n",
        "               .map(parse_function)\n",
        "               .shuffle(batch_size*train_batches, seed=random_seed, reshuffle_each_iteration=reshuffle_training_data)\n",
        "               .batch(batch_size, drop_remainder=True)\n",
        "               .prefetch(train_batches)\n",
        "               .repeat()\n",
        "              )\n",
        "    return dataset.make_one_shot_iterator().get_next()\n",
        "  \n",
        "  \n",
        "def valid_fn(params):\n",
        "    print(params)\n",
        "    valid_tf_record_files = params['valid_tf_record_files']\n",
        "    valid_batches = params['valid_batches']\n",
        "    batch_size = params['batch_size']\n",
        "    files = Dataset.from_tensor_slices(valid_tf_record_files)\n",
        "    dataset = (TFRecordDataset(files)\n",
        "               .map(parse_function)\n",
        "               .batch(batch_size, drop_remainder=True)\n",
        "               .prefetch(valid_batches)\n",
        "              )\n",
        "    return dataset.make_one_shot_iterator().get_next()\n",
        "\n",
        "# sess = tf.Session()\n",
        "# features, labels = train_fn({'batch_size': train_batch_size})\n",
        "# print(sess.run(features))\n",
        "# print(sess.run(labels))\n",
        "# features, labels = valid_fn({'batch_size': valid_batch_size})\n",
        "# print(sess.run(features))\n",
        "# print(sess.run(labels))\n",
        "\n",
        "def mae(labels, predictions):\n",
        "  return tf.metrics.mean(tf.abs(labels - predictions))\n",
        "\n",
        "def last_mae(labels, predictions):\n",
        "    return mae(labels[:,-1], predictions[:,-1])\n",
        "  \n",
        "def mid_mae(labels, predictions):\n",
        "    return mae(labels[:,2047:2049], predictions[:,2047:2049])\n",
        "\n",
        "def seq_mae(labels, predictions):\n",
        "    # seq is 0.0375 secs long so on average we are predicting 0.01875 secs more than the last time step\n",
        "    return mae(labels[:,-1], tf.reduce_mean(predictions, axis=1) - 4.5e-6) \n",
        "  \n",
        "def metric_fn(labels, predictions):\n",
        "  metrics = {\n",
        "    'mae': mae(labels, predictions),\n",
        "    'last_mae': last_mae(labels, predictions),\n",
        "    'mid_mae': mid_mae(labels, predictions),\n",
        "    'seq_mae': seq_mae(labels, predictions),\n",
        "  }\n",
        "#   tf.summary.scalar('mae', metrics['mae'][0])\n",
        "#   tf.summary.scalar('last_mae', metrics['last_mae'][0])\n",
        "#   tf.summary.scalar('seq_mae', metrics['seq_mae'][0])\n",
        "  return metrics\n",
        "  \n",
        "\n",
        "def model_fn(features, labels, mode, params):\n",
        "  filter_width = params['filter_width']\n",
        "  dilation_rates = params['dilation_rates'] \n",
        "  kernel_sizes = params['kernel_sizes']\n",
        "  learning_rate = params['learning_rate']\n",
        "  use_last_mae_loss = params['use_last_mae_loss']\n",
        "  padding = params['padding']\n",
        "  is_training = mode == tf.estimator.ModeKeys.TRAIN\n",
        "  \n",
        "  outputs = tf.layers.conv1d(features, filter_width, 1, activation='relu')\n",
        "  outputs = tf.layers.dropout(outputs, training=is_training)\n",
        "  skip_connection = None\n",
        "  \n",
        "  for kernel_size, dilation_rate in zip(kernel_sizes, dilation_rates):\n",
        "    residual = outputs\n",
        "    outputs = tf.layers.separable_conv1d(outputs, 2*filter_width, kernel_size, dilation_rate=dilation_rate, padding=padding)\n",
        "        \n",
        "    # Gated Linear Units\n",
        "    values = outputs[:,:,:filter_width] # split first half for values\n",
        "    values = tf.tanh(values)\n",
        "    gates = outputs[:,:,filter_width:] # split second half for gates\n",
        "    gates = tf.sigmoid(gates)\n",
        "    outputs = values * gates\n",
        "    outputs = tf.layers.dropout(outputs, training=is_training)\n",
        "    outputs = tf.layers.batch_normalization(outputs)\n",
        "    \n",
        "    skip_connection = outputs if skip_connection == None else skip_connection + outputs\n",
        "    \n",
        "    outputs += residual \n",
        "    \n",
        "  outputs = tf.concat([outputs, skip_connection], axis=-1)\n",
        "  outputs = tf.layers.conv1d(outputs, 2*filter_width, 1, activation='relu')\n",
        "  outputs = tf.layers.dropout(outputs, training=is_training)\n",
        "  last_activation = 'linear' if is_training else 'relu'\n",
        "  outputs = tf.layers.conv1d(outputs, 1, 1, activation=last_activation)\n",
        "  outputs = tf.squeeze(outputs, axis=[-1])\n",
        "                                                               \n",
        "                                                               \n",
        "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "    return tf.estimator.EstimatorSpec(mode, predictions=outputs)\n",
        "  \n",
        "  # Compute loss.\n",
        "#   loss = tf.losses.mean_squared_error(labels, outputs)\n",
        "  loss = None\n",
        "  if use_last_mae_loss:\n",
        "    loss = tf.reduce_mean(tf.abs(labels[-1] - outputs[-1]))\n",
        "  else:\n",
        "    loss = tf.reduce_mean(tf.abs(labels - outputs))\n",
        "  \n",
        "  \n",
        "  if mode == tf.estimator.ModeKeys.EVAL:\n",
        "      return tf.contrib.tpu.TPUEstimatorSpec(mode, loss=loss, eval_metrics=(metric_fn, [labels, outputs]))\n",
        "    \n",
        "      \n",
        "  # Create training op.\n",
        "  assert mode == tf.estimator.ModeKeys.TRAIN\n",
        "\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "  optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
        "  train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
        "  return tf.contrib.tpu.TPUEstimatorSpec(mode, loss=loss, train_op=train_op, eval_metrics=(metric_fn, [labels, outputs]))\n",
        "\n",
        "print(\"Done!\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kGG6MzdppN5m",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title TensorBoard { vertical-output: true }\n",
        "# get_ipython().system_raw(\n",
        "#     'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "#     .format(\"gs://lanl-earthquake-prediction/models\")\n",
        "# )\n",
        "# time.sleep(2)\n",
        "# get_ipython().system_raw('./ngrok http 6007 &')\n",
        "# time.sleep(2)\n",
        "# ! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "#     \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZARYxwzpvqNq",
        "colab_type": "code",
        "outputId": "bf99d645-2283-467d-8e86-d28390f5e0aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1703
        }
      },
      "cell_type": "code",
      "source": [
        "def sendEmbed(url, title, description, inline, params):\n",
        "  try:\n",
        "    discord_webhook = Webhook(url)\n",
        "    embed=Embed(title=title, description=description)\n",
        "    for key, value in params.items():\n",
        "        embed.add_field(name=str(key), value=str(value), inline=inline)\n",
        "    discord_webhook.send(embed=embed)\n",
        "  except:\n",
        "    print('Send embed failed!')\n",
        "\n",
        "for params in experiments:\n",
        "  max_epochs = params['max_epochs']\n",
        "  random_seed = params['random_seed']\n",
        "  epochs_per_validation = params['epochs_per_validation']\n",
        "  early_stopping_epsilon = params['early_stopping_epsilon']\n",
        "  experiment_name = params['experiment_name']\n",
        "  cross_validation_sets = params['cross_validation_sets']\n",
        "  \n",
        "  max_cross_validation_steps = (train_earthquakes+valid_earthquakes) // valid_earthquakes\n",
        "  time_string = time.strftime('%Y-%m-%d-%H-%M-%S')\n",
        "  tf_record_files = [\"{0}eq_{1}_packets.tfrecord\".format(DATASET_PATH, i) for i in range(total_earthquakes)]\n",
        " \n",
        "  sendEmbed(discord_webhook_url, 'Experiment started!', '', True, params)\n",
        "    \n",
        "#   try:\n",
        "  for cv_step in cross_validation_sets:\n",
        "    cv_shift = cv_step * valid_earthquakes\n",
        "    cv_params = dict(params)\n",
        "    cv_packets = list(packets[:-test_earthquakes])\n",
        "    cv_packets = cv_packets[-cv_shift:] + cv_packets[:-cv_shift] # roll\n",
        "    cv_tf_record_files = list(tf_record_files[:-test_earthquakes])\n",
        "    cv_tf_record_files = cv_tf_record_files[-cv_shift:] + cv_tf_record_files[:-cv_shift]  # roll\n",
        "\n",
        "    cv_params['train_packets'] = sum(cv_packets[:train_earthquakes])\n",
        "    cv_params['valid_packets'] = sum(cv_packets[train_earthquakes:])\n",
        "    cv_params['train_tf_record_files'] = cv_tf_record_files[:train_earthquakes]\n",
        "    cv_params['valid_tf_record_files'] = cv_tf_record_files[train_earthquakes:]\n",
        "\n",
        "    train_batches = cv_params['train_packets'] // train_batch_size\n",
        "    valid_batches = cv_params['valid_packets'] // valid_batch_size\n",
        "    cv_params['train_batches'] = train_batches\n",
        "    cv_params['valid_batches'] = valid_batches\n",
        "\n",
        "    MODEL_DIR = 'gs://{}/models/{}/{}/{}_{}'.format(bucket, time_string, model_name, experiment_name, cv_step)\n",
        "\n",
        "    train_steps = epochs_per_validation*train_batches*train_batch_size\n",
        "\n",
        "    config = tf.contrib.tpu.RunConfig(\n",
        "        tf_random_seed=random_seed,\n",
        "        master=TF_MASTER,\n",
        "        model_dir=MODEL_DIR,\n",
        "        save_summary_steps=train_steps,\n",
        "        save_checkpoints_steps=train_steps,\n",
        "        tpu_config=tf.contrib.tpu.TPUConfig(num_shards=8, iterations_per_loop=train_batches))\n",
        "\n",
        "    regressor = tf.contrib.tpu.TPUEstimator(\n",
        "        model_fn=model_fn,\n",
        "        params=cv_params,\n",
        "        config=config,\n",
        "        train_batch_size = train_batch_size,\n",
        "        eval_batch_size = valid_batch_size,\n",
        "        eval_on_tpu=use_tpu,\n",
        "        use_tpu=use_tpu)\n",
        "\n",
        "    sendEmbed(discord_webhook_url, 'Training started!', '', True, cv_params)\n",
        "      \n",
        "    last_last_mae = None\n",
        "    epoch = 0\n",
        "    while epoch < max_epochs:\n",
        "      # Train the Model.\n",
        "      regressor.train(input_fn=train_fn, steps=epochs_per_validation*train_batches)\n",
        "\n",
        "\n",
        "      epoch += epochs_per_validation\n",
        "\n",
        "      # Evaluate the model.\n",
        "      eval_result = regressor.evaluate(input_fn=valid_fn, steps=valid_batches)\n",
        "      discord_message = 'Experiment: {0} Epoch: {1}\\n{2}'.format(experiment_name, epoch, eval_result)\n",
        "      sendEmbed(discord_webhook_url, 'Validation', discord_message, True, eval_result)\n",
        "      if last_last_mae and eval_result['last_mae'] > last_last_mae + early_stopping_epsilon:\n",
        "        break\n",
        "      elif last_last_mae:\n",
        "        last_last_mae = min(eval_result['last_mae'], last_last_mae)\n",
        "      else:\n",
        "        last_last_mae = eval_result['last_mae']\n",
        "#   except Exception as e:\n",
        "#     print(e)\n",
        "#     discord_webhook.send(\"Experament {} faild!\\n{}\".format(experiment_name, e))\n",
        "  \n",
        "discord_webhook.send(\"Training stopped!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://lanl-earthquake-prediction/models/2019-02-07-07-44-48/dcnn-p-e2e-tf/cv-x4_2', '_tf_random_seed': 42, '_save_summary_steps': 3375360, '_save_checkpoints_steps': 3375360, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa8e2519518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.26.13.154:8470', '_evaluation_master': 'grpc://10.26.13.154:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=879, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None), '_cluster': None}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.26.13.154:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 16832764385906886893)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 11884467485365774112)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 4831294019828008607)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 2500098388126251820)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 10128368460189152667)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 5068711343869401661)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 42833022162495869)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 6046325417978066240)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 17868592339643487845)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 5839991267622860419)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 12107746342976545239)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "{'experiment_name': 'cv-x4', 'description': 'cross validating experiment 4 - shuffle all packets but without reshuffling every epoch', 'learning_rate': 0.00125, 'dilation_rates': [1, 4, 16, 64, 256, 1024], 'kernel_sizes': [4, 4, 4, 4, 4, 4], 'filter_width': 32, 'random_seed': 42, 'max_epochs': 120, 'epochs_per_validation': 30, 'early_stopping_epsilon': 0.5, 'use_last_mae_loss': False, 'padding': 'causal', 'cross_validation_sets': [2, 3, 4, 5, 6, 0, 1], 'reshuffle_training_data': True, 'train_packets': 112519, 'valid_packets': 16476, 'train_tf_record_files': ['gs://lanl-earthquake-prediction/datasets/eq_10_packets.tfrecord', 'gs://lanl-earthquake-prediction/datasets/eq_11_packets.tfrecord', 'gs://lanl-earthquake-prediction/datasets/eq_12_packets.tfrecord', 'gs://lanl-earthquake-prediction/datasets/eq_13_packets.tfrecord', 'gs://lanl-earthquake-prediction/datasets/eq_0_packets.tfrecord', 'gs://lanl-earthquake-prediction/datasets/eq_1_packets.tfrecord', 'gs://lanl-earthquake-prediction/datasets/eq_2_packets.tfrecord', 'gs://lanl-earthquake-prediction/datasets/eq_3_packets.tfrecord', 'gs://lanl-earthquake-prediction/datasets/eq_4_packets.tfrecord', 'gs://lanl-earthquake-prediction/datasets/eq_5_packets.tfrecord', 'gs://lanl-earthquake-prediction/datasets/eq_6_packets.tfrecord', 'gs://lanl-earthquake-prediction/datasets/eq_7_packets.tfrecord'], 'valid_tf_record_files': ['gs://lanl-earthquake-prediction/datasets/eq_8_packets.tfrecord', 'gs://lanl-earthquake-prediction/datasets/eq_9_packets.tfrecord'], 'train_batches': 879, 'valid_batches': 128, 'batch_size': 128, 'context': <tensorflow.contrib.tpu.python.tpu.tpu_context.TPUContext object at 0x7fa8e1c5d438>}\n",
            "WARNING:tensorflow:From <ipython-input-3-7385dc623427>:29: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
            "WARNING:tensorflow:From <ipython-input-3-7385dc623427>:88: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv1d instead.\n",
            "WARNING:tensorflow:From <ipython-input-3-7385dc623427>:89: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-3-7385dc623427>:94: separable_conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.separable_conv1d instead.\n",
            "WARNING:tensorflow:From <ipython-input-3-7385dc623427>:103: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.batch_normalization instead.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:TPU job name tpu_worker\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into gs://lanl-earthquake-prediction/models/2019-02-07-07-44-48/dcnn-p-e2e-tf/cv-x4_2/model.ckpt.\n",
            "INFO:tensorflow:Initialized dataset iterators in 0 seconds\n",
            "INFO:tensorflow:Installing graceful shutdown hook.\n",
            "INFO:tensorflow:Creating heartbeat manager for ['/job:tpu_worker/replica:0/task:0/device:CPU:0']\n",
            "INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "INFO:tensorflow:Init TPU system\n",
            "INFO:tensorflow:Initialized TPU in 3 seconds\n",
            "INFO:tensorflow:Starting infeed thread controller.\n",
            "INFO:tensorflow:Starting outfeed thread controller.\n",
            "INFO:tensorflow:Enqueue next (879) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (879) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 2.6462746, step = 879\n",
            "INFO:tensorflow:Enqueue next (879) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (879) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 2.4078805, step = 1758 (45.279 sec)\n",
            "INFO:tensorflow:global_step/sec: 19.4138\n",
            "INFO:tensorflow:examples/sec: 2484.97\n",
            "INFO:tensorflow:Enqueue next (879) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (879) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 2.7825027, step = 2637 (45.437 sec)\n",
            "INFO:tensorflow:global_step/sec: 19.3456\n",
            "INFO:tensorflow:examples/sec: 2476.24\n",
            "INFO:tensorflow:Enqueue next (879) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (879) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 3.2510297, step = 3516 (45.551 sec)\n",
            "INFO:tensorflow:global_step/sec: 19.2972\n",
            "INFO:tensorflow:examples/sec: 2470.04\n",
            "INFO:tensorflow:Enqueue next (879) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (879) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 2.7600675, step = 4395 (44.497 sec)\n",
            "INFO:tensorflow:global_step/sec: 19.754\n",
            "INFO:tensorflow:examples/sec: 2528.51\n",
            "INFO:tensorflow:Enqueue next (879) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (879) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 2.7383745, step = 5274 (43.994 sec)\n",
            "INFO:tensorflow:global_step/sec: 19.9798\n",
            "INFO:tensorflow:examples/sec: 2557.42\n",
            "INFO:tensorflow:Enqueue next (879) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (879) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 3.765292, step = 6153 (44.348 sec)\n",
            "INFO:tensorflow:global_step/sec: 19.8205\n",
            "INFO:tensorflow:examples/sec: 2537.03\n",
            "INFO:tensorflow:Enqueue next (879) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (879) batch(es) of data from outfeed.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}