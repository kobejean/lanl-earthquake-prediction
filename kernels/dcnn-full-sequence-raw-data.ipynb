{
  "cells": [
    {
      "metadata": {
        "_uuid": "24182f879a9e403e382a0c3b6bfc92bb995fceaf"
      },
      "cell_type": "markdown",
      "source": "# DCNN Full Sequence Feature Engineered\nThis model takes the 150,000 rows of acoustic data, and takes the mean, standard deviation etc. of every 128 datapoints (granularity=128) to make our features. Then train a WaveNet-like dialated causal convolutional neural network on these features to predict the time remaining before a lab earthquake. This model is trained to make predictions for each time step. When we are making our test data predictions we use the average of these predictions to make a single prediction of the remaining time at the last time step."
    },
    {
      "metadata": {
        "_uuid": "7aaf8f776224015089cf92a51d99216d2aef63c5"
      },
      "cell_type": "markdown",
      "source": "## Import python modules"
    },
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": true,
        "_kg_hide-output": false,
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nfrom threading import Thread\n\nimport os\nimport sys\nimport glob\nimport random\nprint(\"Imported!\")",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Imported!\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "668f1f7eb0bf9f6beb3a9f2849a0d90d3ef448da"
      },
      "cell_type": "markdown",
      "source": "## Set Hyperparameters/Constants\nAll our hyperparameters and global variables in one place."
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "f9cf5fa9ed66cddc694419d1b572da63507abe0b",
        "trusted": true
      },
      "cell_type": "code",
      "source": "model_name = 'dcnn-full-sequence-end-to-end'\ntotal_size = 629_145_480\nseg_size = 150_000\nbin_size = 2**12 # data comes in bins of 2^12 contiguous rows (ADC with 12-bit resolution)\nbins_per_seg = (seg_size + bin_size - 1) // bin_size # ceil(seg_size / bin_size)\ncase_size = bins_per_seg * bin_size\ntarget_size = 40\ngranularity = 128\nseq_size = (seg_size + granularity -1) // granularity\n\nepochs = 18 #16 * bins_per_seg\nbatch_size = 2**3\nfeature_size = 16\nvalid_steps = 32\nlearning_rate = 0.002\n\ntotal_cases = total_size // case_size\ntotal_batches = (total_cases + batch_size - 1) // batch_size\n\nwithheld_cases = 2**8\nwithheld_batches = (withheld_cases + batch_size - 1) // batch_size\nwithheld_size = withheld_cases * case_size\nwithheld_percent = 100 * withheld_size / total_size\n\nvalid_cases = max(valid_steps * batch_size, valid_steps * 32)\nvalid_batches = valid_cases // batch_size\nvalid_size = valid_cases * case_size\nvalid_percent = 100 * valid_size / total_size\n\ntrain_size = total_size - valid_size - withheld_size\ntrain_cases = total_cases - valid_cases - withheld_cases\ntrain_batches = train_cases // batch_size\ntrain_percent = 100 * train_size / total_size\n\n\nprint(\"----- Hyperparameters -----\")\nprint(\"epochs:\", epochs)\nprint(\"batch_size:\", batch_size)\nprint(\"feature_size:\", feature_size)\nprint(\"valid_steps:\", valid_steps)\nprint(\"learning_rate:\", learning_rate)\nprint(\"---------------------------\")\nprint(\"seg_size:\", seg_size)\nprint(\"bin_size:\", bin_size)\nprint(\"bins_per_seg:\", bins_per_seg)\nprint(\"case_size:\", case_size)\nprint(\"target_size:\", target_size)\nprint(\"granularity:\", granularity)\nprint(\"seq_size:\", seq_size)\nprint(\"total_cases:\", total_cases)\nprint(\"total_batches:\", total_batches)\nprint(\"withheld_cases:\", withheld_cases)\nprint(\"withheld_batches:\", withheld_batches)\nprint(\"valid_cases:\", valid_cases)\nprint(\"valid_batches:\", valid_batches)\nprint(\"train_cases:\", train_cases)\nprint(\"train_batches:\", train_batches)\nprint(\"---------------------------\")\nprint(\"total_size:       {0:9d}\".format(total_size))\nprint(\"withheld_size:    {0:9d}\".format(withheld_size))\nprint(\"train_size:       {0:9d}\".format(train_size))\nprint(\"valid_size:       {0:9d}\".format(valid_size))\nprint(\"---------------------------\")\nprint(\"withheld_percent: {0:8.2f}%\".format(withheld_percent))\nprint(\"valid_percent:    {0:8.2f}%\".format(valid_percent))\nprint(\"train_percent:    {0:8.2f}%\".format(train_percent))\nprint(\"---------------------------\")\nprint(\"model_name:\", model_name)\nprint(\"---------------------------\")",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": "----- Hyperparameters -----\nepochs: 18\nbatch_size: 8\nfeature_size: 16\nvalid_steps: 32\nlearning_rate: 0.002\n---------------------------\nseg_size: 150000\nbin_size: 4096\nbins_per_seg: 37\ncase_size: 151552\ntarget_size: 40\ngranularity: 128\nseq_size: 1172\ntotal_cases: 4151\ntotal_batches: 519\nwithheld_cases: 256\nwithheld_batches: 32\nvalid_cases: 1024\nvalid_batches: 128\ntrain_cases: 2871\ntrain_batches: 358\n---------------------------\ntotal_size:       629145480\nwithheld_size:     38797312\ntrain_size:       435158920\nvalid_size:       155189248\n---------------------------\nwithheld_percent:     6.17%\nvalid_percent:       24.67%\ntrain_percent:       69.17%\n---------------------------\nmodel_name: dcnn-full-sequence-end-to-end\n---------------------------\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "6621873b6dc141e501087edc1d013a5a28ed6139"
      },
      "cell_type": "markdown",
      "source": "## Load data\nThis takes a while to load the data into memory but it only needs to be run once every jupyter notebook session."
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "bfa2da16e1ac711eef7274cdaeed68d01a2d9028",
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Reading data...\")\ndata = pd.read_csv('../input/train.csv', dtype={'acoustic_data': np.float32, 'time_to_failure': np.float32})[:-withheld_size]\nprint(\"Done reading data!\")",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Reading data...\nDone reading data!\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "7b4be734ff8a21aa6a10e3624bc56755933e3abe"
      },
      "cell_type": "markdown",
      "source": "## Data Augmentation/Feature Engineering\nHere we do our feature engineering and define our batch generator."
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# A vector with 1's marking the end of a bin\nbin_end = np.zeros((1, bin_size), dtype=np.float32)\nbin_end[0,-1] = 1.0\n\ndef create_features(df):\n    gran_mean = (df.mean(axis=1) - 4.47) / 1.5\n    gran_median = (np.median(np.abs(df), axis=1) - 4.5) / 1.5\n    gran_std = (df.std(axis=1) - 4.26) / 9.7\n    gran_max = (df.max(axis=1) - 13.83) / 19.0\n    gran_min = (df.min(axis=1) + 4.83) / 18.6\n    return np.c_[gran_mean, gran_median, gran_std, gran_max, gran_min]\n\ndef augment_data(df):\n    x = df.acoustic_data.values\n    x = np.expand_dims(x, axis=1)\n    bin_end_resized = np.resize(bin_end, (x.shape[0],1))\n    x = np.concatenate((x, bin_end_resized), axis=1)\n    return x\n\ndef batch_gen(validation=False):\n    while True:\n        shifts = [36] if validation else [0,18,9,27,4,22,13,31,6,24,15,33,2,20,11,29,17,35,7,25,1,19,10,28,3,21,12,30,5,23,14,32,8,26,16,34,36]\n#         shifts = [0,18,9,27,4,22,13,31,6,24,15,33,2,20,11,29,17,35,7,25,1,19,10,28,3,21,12,30,5,23,14,32,8,26,16,34,36]\n        for shift in shifts:\n            batches = valid_batches if validation else train_batches\n            cases = valid_cases if validation else train_cases\n            cases_start = train_cases if validation else 0\n            case_idxs = random.sample(list(range(cases_start, cases_start+cases)), cases)\n            \n            for batch in range(batches):\n                start_case = batch * batch_size\n                end_case = start_case + batch_size\n                samples = []\n                targets = []\n                for i, case in enumerate(case_idxs[start_case:end_case]):\n                    start_row = case * case_size + shift * bin_size\n                    end_row = start_row + seg_size\n                    seg = data[start_row:end_row]\n                    if seg.shape[0] == seg_size:\n                        features = augment_data(seg)\n                        samples.append(features)\n                        y = np.expand_dims(seg.time_to_failure.values, axis=1)\n                        targets.append(y)\n                yield np.asarray(samples), np.asarray(targets)\n\ntrain = batch_gen()\nvalid = batch_gen(validation=True)",
      "execution_count": 21,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b3384ba8e764f90aec2cf65aec50fbebf7ffe0c5"
      },
      "cell_type": "markdown",
      "source": "## Custom Metrics\nAdd some custom metrics to monitor our model. \n- `last_mae` is the mean absolute error of the last prediction. Since our model is going to be judged by how well it can predict the time before an earthquake *at the last time step*, this might be nice to monitor. It should reflect how well the model can use the widest receptive field for the prediction.\n- `seq_mae` is the mean absolute error of the last prediction using the mean of all predictions calibrated to predict the last time step. Since the test data comes in 0.0375 sec sequences we should expect the mean of the predictions to be 0.01875 secs behind the last prediction (or ahead in terms of remaining time). Thus taking the mean of our predictions and subtracting 0.01875 secs should give us a pretty good prediction of the remaining time at the last time step.\n\nIt would be interesting to see the distibution of the mean absolute error for each relative time step. If we could see at what relative time step the model performs the the best, it could give us an idea of how well the model performs relative to the size of the receptive field. If there is a way we could implement a `nth_step_mae` variant of `last_mae` that would be cool."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "41990e10c36fc7123dcc963597fa51719e128eb1"
      },
      "cell_type": "code",
      "source": "from keras.losses import mean_absolute_error\nfrom keras.callbacks import Callback\nfrom keras.backend import mean\n\ndef last_mae(y_true, y_pred):\n    return mean_absolute_error(y_true[:,-1], y_pred[:,-1])\n\ndef seq_mae(y_true, y_pred):\n    # seq is 0.0375 secs long so on average we are predicting 0.01875 secs more than the last time step\n    return mean_absolute_error(y_true[:,-1], mean(y_pred, axis=1) - 0.01875) ",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "d657c7788d3145b65f0758aa9788b3146befb530"
      },
      "cell_type": "markdown",
      "source": "## TensorBoard\nTODO: Launch TensorBoard on Kaggle "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "665aff40b3f177a891276349f1ccccd3bd2b5d34"
      },
      "cell_type": "code",
      "source": "# TODO: Launch TensorBoard on Kaggle \n# https://www.kaggle.com/shivam1600/tensorboard-on-kaggle-very-concise-part-1\n# # At first in settings, Make sure that Internet option is set to \"Internet Connected\"\n# # After executing this cell, there will come a link below, open that to view your tensor-board\n\n# !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n# !unzip ngrok-stable-linux-amd64.zip\n# LOG_DIR = './logs' # Here you have to put your log directory\n# get_ipython().system_raw(\n#     'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n#     .format(LOG_DIR)\n# )\n# get_ipython().system_raw('./ngrok http 6006 &')\n# ! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n#     \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a2192a9ef4352a836c154330611990bfad693f92"
      },
      "cell_type": "markdown",
      "source": "## Define Model"
    },
    {
      "metadata": {
        "_uuid": "76c0b1555461c08b2f68cde4566455eafe2fde00",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Define model\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM, CuDNNGRU, Concatenate, Multiply, Input, Lambda\nfrom keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D, AveragePooling1D, BatchNormalization, Activation, LeakyReLU, ReLU, Add\nfrom keras.optimizers import adam\nfrom keras.callbacks import ModelCheckpoint, TensorBoard\nfrom keras.models import Model\nfrom keras import regularizers\nfrom time import time\n\nfrom keras.layers import Layer\n\ncb = [ModelCheckpoint(model_name + '-{epoch:03d}-val_last_mae{val_last_mae:.2f}.hdf5', monitor='val_last_mae',\n                      mode='min', save_weights_only=False, period=9)]\nif True:\n    cb += [TensorBoard(log_dir=\"logs/{}\".format(time()), update_freq='epoch')]\n    \n# define model\ndilations =    [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16_384, 49_152]\nfilter_width =  12\nkernel_sizes = [2, 2, 2, 2,  2,  2,  2,   2,   2,   2,    2,    2,    2,    2,      3,      3]\ndilated_layers = len(dilations)\n\ninputs = Input((seg_size, 2))\nskip_connection = None\noutputs = Conv1D(filter_width, 1)(inputs)\nfor i, dilation, kernel_size in zip(range(dilated_layers), dilations, kernel_sizes):\n    residual = outputs\n    outputs = Conv1D(2*filter_width, kernel_size, dilation_rate=dilation, padding='causal')(outputs)\n    \n    # Gated Linear Units\n    values = Lambda(lambda x: x[:,:,:filter_width])(outputs) # split first half for values\n    values = Activation(activation='tanh')(values)\n    gates = Lambda(lambda x: x[:,:,filter_width:])(outputs) # split second half for gates\n    gates = Activation(activation='sigmoid')(gates)\n    outputs = Multiply()([values, gates])\n    outputs = BatchNormalization()(outputs)\n#     outputs = Dropout(0.2)(outputs)\n\n    if i < dilated_layers-1:\n        skip_connection = outputs if skip_connection == None else Add()([skip_connection, outputs])\n    \n    outputs = Add()([residual, outputs])\n\nskip_connection = Activation(activation='relu')(skip_connection)\n\noutputs = Conv1D(filter_width, 1, activation='relu')(outputs)\noutputs = Concatenate(axis=-1)([outputs, skip_connection])\noutputs = Conv1D(1, 1, activation='relu')(outputs)\n\nmodel = Model(inputs=inputs, outputs=outputs)\n\nmodel.summary()\n\n# Compile and fit model\nmodel.compile(optimizer=adam(lr=learning_rate), loss=\"mae\", metrics=[last_mae, seq_mae])\n\nhistory = model.fit_generator(train,\n                              steps_per_epoch=train_batches,\n                              epochs=epochs,\n                              verbose=1,\n                              callbacks=cb,\n                              validation_data=valid,\n                              validation_steps=valid_steps)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1a8c972d212557beb2f7550c02e62663f872a8ce",
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\n# Visualize accuracies\nimport matplotlib.pyplot as plt\n\ndef perf_plot(history, what = 'last_mae'):\n    x = history.history[what]\n    val_x = history.history['val_' + what]\n    epochs = np.asarray(history.epoch) + 1\n\n    plt.plot(epochs, x, 'bo', label = \"Training \" + what)\n    plt.plot(epochs, val_x, 'b', label = \"Validation \" + what)\n    plt.title(\"Training and validation \" + what)\n    plt.xlabel(\"Epochs\")\n    plt.legend()\n    plt.show()\n    return None\n\nperf_plot(history, 'loss')\nperf_plot(history, 'last_mae')\nperf_plot(history, 'seq_mae')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3b6ad90328ba3962ede6d26ec5018c72f734f827",
        "trusted": true
      },
      "cell_type": "code",
      "source": "submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id', dtype={\"time_to_failure\": np.float32})\n\n# Load each test data, create the feature matrix, get numeric prediction\nfor i, seg_id in enumerate(tqdm(submission.index, total=submission.shape[0])):\n    seg = pd.read_csv('../input/test/' + seg_id + '.csv', dtype={'acoustic_data': np.float32})\n    features = augment_data(seg.acoustic_data.values)\n    predict = model.predict(features).mean(axis=1) - 0.01875\n    submission.time_to_failure[i] = predict\n\nsubmission.head()\n\n# Save\nsubmission.to_csv('submission.csv')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1024b06fd1c95b1ad39c943fd9427871c0bda4fe",
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}