{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported!\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "from threading import Thread\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "print(\"Imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Hyperparameters -----\n",
      "epochs: 1\n",
      "batch_size: 64\n",
      "feature_size: 2\n",
      "valid_steps: 2\n",
      "---------------------------\n",
      "seg_size: 150000\n",
      "bin_size: 4096\n",
      "bins_per_seg: 37\n",
      "case_size: 151552\n",
      "total_cases: 4151\n",
      "total_batches: 65\n",
      "withheld_cases: 256\n",
      "withheld_batches: 4\n",
      "valid_cases: 128\n",
      "valid_batches: 2\n",
      "train_cases: 3767\n",
      "train_batches: 59\n",
      "---------------------------\n",
      "total_size:       629145480\n",
      "withheld_size:     38797312\n",
      "train_size:       570949512\n",
      "valid_size:        19398656\n",
      "---------------------------\n",
      "withheld_percent:     6.17%\n",
      "valid_percent:        3.08%\n",
      "train_percent:       90.75%\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "batch_size = 2**6\n",
    "feature_size = 2\n",
    "valid_steps = 2\n",
    "\n",
    "total_size = 629_145_480\n",
    "seg_size = 150_000\n",
    "bin_size = 2**12 # data comes in bins of 2^12 contiguous rows (ADC with 12-bit resolution)\n",
    "bins_per_seg = (seg_size + bin_size - 1) // bin_size # ceil(seg_size / bin_size)\n",
    "case_size = bins_per_seg * bin_size\n",
    "\n",
    "total_cases = total_size // case_size\n",
    "total_batches = (total_cases + batch_size - 1) // batch_size\n",
    "\n",
    "withheld_cases = 2**8\n",
    "withheld_batches = (withheld_cases + batch_size - 1) // batch_size\n",
    "withheld_size = withheld_cases * case_size\n",
    "withheld_percent = 100 * withheld_size / total_size\n",
    "\n",
    "valid_cases = valid_steps * batch_size\n",
    "valid_batches = (valid_cases + batch_size - 1) // batch_size\n",
    "valid_size = valid_cases * case_size\n",
    "valid_percent = 100 * valid_size / total_size\n",
    "\n",
    "train_size = total_size - valid_size - withheld_size\n",
    "train_cases = total_cases - valid_cases - withheld_cases\n",
    "train_batches = (train_cases + batch_size - 1) // batch_size\n",
    "train_percent = 100 * train_size / total_size\n",
    "\n",
    "print(\"----- Hyperparameters -----\")\n",
    "print(\"epochs:\", epochs)\n",
    "print(\"batch_size:\", batch_size)\n",
    "print(\"feature_size:\", feature_size)\n",
    "print(\"valid_steps:\", valid_steps)\n",
    "print(\"---------------------------\")\n",
    "print(\"seg_size:\", seg_size)\n",
    "print(\"bin_size:\", bin_size)\n",
    "print(\"bins_per_seg:\", bins_per_seg)\n",
    "print(\"case_size:\", case_size)\n",
    "print(\"total_cases:\", total_cases)\n",
    "print(\"total_batches:\", total_batches)\n",
    "print(\"withheld_cases:\", withheld_cases)\n",
    "print(\"withheld_batches:\", withheld_batches)\n",
    "print(\"valid_cases:\", valid_cases)\n",
    "print(\"valid_batches:\", valid_batches)\n",
    "print(\"train_cases:\", train_cases)\n",
    "print(\"train_batches:\", train_batches)\n",
    "print(\"---------------------------\")\n",
    "print(\"total_size:       {0:9d}\".format(total_size))\n",
    "print(\"withheld_size:    {0:9d}\".format(withheld_size))\n",
    "print(\"train_size:       {0:9d}\".format(train_size))\n",
    "print(\"valid_size:       {0:9d}\".format(valid_size))\n",
    "print(\"---------------------------\")\n",
    "print(\"withheld_percent: {0:8.2f}%\".format(withheld_percent))\n",
    "print(\"valid_percent:    {0:8.2f}%\".format(valid_percent))\n",
    "print(\"train_percent:    {0:8.2f}%\".format(train_percent))\n",
    "print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Done reading data!\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading data...\")\n",
    "data = pd.read_csv('../input/train.csv', dtype={'acoustic_data': np.float32, 'time_to_failure': np.float32})[:-withheld_size]\n",
    "print(\"Done reading data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# A vector with 1's marking the end of a bin\n",
    "bin_end = np.zeros((1, bin_size), dtype=np.float32)\n",
    "bin_end[0,-1] = 1.0\n",
    "\n",
    "def create_features(df):\n",
    "    df_size = df.shape[0]\n",
    "    x = np.zeros((df_size, 2))\n",
    "    x[:, 0] = (df.acoustic_data.values - 4.51946757) / 10.7357072\n",
    "    x[:, 1] = np.resize(bin_end, (1, df_size))\n",
    "    return x\n",
    "\n",
    "def batch_gen(validation=False):\n",
    "    samples = np.zeros((batch_size, seg_size, feature_size))\n",
    "    targets = np.zeros((batch_size, 1, 1))\n",
    "    while True:\n",
    "        start_case = train_cases if validation else 0\n",
    "        end_case = train_cases + valid_cases if validation else train_cases\n",
    "        cases = np.random.randint(start_case, end_case, size=batch_size)\n",
    "\n",
    "        for i, case in enumerate(cases):\n",
    "            start_row = case * case_size\n",
    "            end_row = start_row + seg_size\n",
    "            seg = data[start_row:end_row]\n",
    "            samples[i] = create_features(seg)\n",
    "            targets[i] = seg.tail(1).time_to_failure.values\n",
    "        yield samples, targets\n",
    "\n",
    "train = batch_gen()\n",
    "valid = batch_gen(validation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[[ 0.32420152,  0.        ],\n",
      "        [ 0.04476022,  0.        ],\n",
      "        [ 0.04476022,  0.        ],\n",
      "        ...,\n",
      "        [-0.23468108,  0.        ],\n",
      "        [-0.32782817,  0.        ],\n",
      "        [-0.32782817,  0.        ]],\n",
      "\n",
      "       [[ 0.51049572,  0.        ],\n",
      "        [ 0.41734862,  0.        ],\n",
      "        [-0.23468108,  0.        ],\n",
      "        ...,\n",
      "        [ 0.23105443,  0.        ],\n",
      "        [ 0.60364282,  0.        ],\n",
      "        [ 0.23105443,  0.        ]],\n",
      "\n",
      "       [[ 0.78993702,  0.        ],\n",
      "        [ 0.69678992,  0.        ],\n",
      "        [ 0.32420152,  0.        ],\n",
      "        ...,\n",
      "        [-0.51412237,  0.        ],\n",
      "        [-0.14153397,  0.        ],\n",
      "        [-0.23468108,  0.        ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 0.23105443,  0.        ],\n",
      "        [ 0.04476022,  0.        ],\n",
      "        [-0.14153397,  0.        ],\n",
      "        ...,\n",
      "        [ 0.13790733,  0.        ],\n",
      "        [-0.14153397,  0.        ],\n",
      "        [-0.14153397,  0.        ]],\n",
      "\n",
      "       [[-0.14153397,  0.        ],\n",
      "        [ 0.32420152,  0.        ],\n",
      "        [ 0.13790733,  0.        ],\n",
      "        ...,\n",
      "        [-0.04838688,  0.        ],\n",
      "        [-0.04838688,  0.        ],\n",
      "        [ 0.04476022,  0.        ]],\n",
      "\n",
      "       [[ 0.04476022,  0.        ],\n",
      "        [-0.04838688,  0.        ],\n",
      "        [ 0.04476022,  0.        ],\n",
      "        ...,\n",
      "        [ 0.13790733,  0.        ],\n",
      "        [ 0.32420152,  0.        ],\n",
      "        [ 0.41734862,  0.        ]]]), array([[[2.43399715]],\n",
      "\n",
      "       [[8.3901968 ]],\n",
      "\n",
      "       [[1.96159708]],\n",
      "\n",
      "       [[2.43399715]],\n",
      "\n",
      "       [[8.98079681]],\n",
      "\n",
      "       [[2.31589699]],\n",
      "\n",
      "       [[2.35519719]],\n",
      "\n",
      "       [[3.61499715]],\n",
      "\n",
      "       [[2.866997  ]],\n",
      "\n",
      "       [[2.1189971 ]],\n",
      "\n",
      "       [[8.3901968 ]],\n",
      "\n",
      "       [[2.82759714]],\n",
      "\n",
      "       [[3.02449703]],\n",
      "\n",
      "       [[9.45319748]],\n",
      "\n",
      "       [[9.45319748]],\n",
      "\n",
      "       [[3.02449703]],\n",
      "\n",
      "       [[9.41379738]],\n",
      "\n",
      "       [[1.33169711]],\n",
      "\n",
      "       [[1.5284971 ]],\n",
      "\n",
      "       [[1.96159708]],\n",
      "\n",
      "       [[3.1425972 ]],\n",
      "\n",
      "       [[8.50829697]],\n",
      "\n",
      "       [[1.56789708]],\n",
      "\n",
      "       [[3.61499715]],\n",
      "\n",
      "       [[2.51269698]],\n",
      "\n",
      "       [[0.97739708]],\n",
      "\n",
      "       [[0.26869708]],\n",
      "\n",
      "       [[1.41039705]],\n",
      "\n",
      "       [[3.57559705]],\n",
      "\n",
      "       [[2.07969713]],\n",
      "\n",
      "       [[2.51269698]],\n",
      "\n",
      "       [[8.3901968 ]],\n",
      "\n",
      "       [[1.68599713]],\n",
      "\n",
      "       [[3.45749712]],\n",
      "\n",
      "       [[8.66579723]],\n",
      "\n",
      "       [[8.23279667]],\n",
      "\n",
      "       [[2.67019701]],\n",
      "\n",
      "       [[8.35089684]],\n",
      "\n",
      "       [[0.11129709]],\n",
      "\n",
      "       [[0.3867971 ]],\n",
      "\n",
      "       [[3.57559705]],\n",
      "\n",
      "       [[0.8592971 ]],\n",
      "\n",
      "       [[8.468997  ]],\n",
      "\n",
      "       [[1.25289714]],\n",
      "\n",
      "       [[1.1347971 ]],\n",
      "\n",
      "       [[9.02009678]],\n",
      "\n",
      "       [[3.41819715]],\n",
      "\n",
      "       [[0.0718971 ]],\n",
      "\n",
      "       [[2.59139705]],\n",
      "\n",
      "       [[2.94569707]],\n",
      "\n",
      "       [[2.55209708]],\n",
      "\n",
      "       [[2.9063971 ]],\n",
      "\n",
      "       [[2.43399715]],\n",
      "\n",
      "       [[9.09889698]],\n",
      "\n",
      "       [[1.84339714]],\n",
      "\n",
      "       [[9.02009678]],\n",
      "\n",
      "       [[8.4295969 ]],\n",
      "\n",
      "       [[1.88279712]],\n",
      "\n",
      "       [[2.00089717]],\n",
      "\n",
      "       [[3.45749712]],\n",
      "\n",
      "       [[2.70949721]],\n",
      "\n",
      "       [[0.11129709]],\n",
      "\n",
      "       [[3.30009699]],\n",
      "\n",
      "       [[0.03249709]]]))\n"
     ]
    }
   ],
   "source": [
    "print(next(valid))\n",
    "# rows = np.random.randint(0, train_size, size=batch_size)\n",
    "# print(rows, data.shape)\n",
    "# data[rows[0]:rows[0]+seg_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "76c0b1555461c08b2f68cde4566455eafe2fde00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_14 (Conv1D)           (None, 37500, 4)          36        \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 9375, 8)           136       \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 1875, 16)          656       \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 375, 32)           2592      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 25, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 5, 64)             10304     \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 1, 1)              321       \n",
      "=================================================================\n",
      "Total params: 14,045\n",
      "Trainable params: 14,045\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.optimizers import adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "cb = [ModelCheckpoint(\"dcnn-model.hdf5\", monitor='val_loss', save_weights_only=False, period=3)]\n",
    "\n",
    "# 25, 25, 16, 5, 3\n",
    "model = Sequential()\n",
    "model.add(Conv1D(4, 4, activation='relu'  , dilation_rate=1, strides=4, input_shape=(seg_size, feature_size), padding=\"valid\"))\n",
    "model.add(Conv1D(8, 4, activation='relu'  , dilation_rate=1, strides=4, input_shape=(None, 4), padding=\"valid\"))\n",
    "model.add(Conv1D(16, 5, activation='relu'  , dilation_rate=1, strides=5, input_shape=(None, 8), padding=\"valid\"))\n",
    "model.add(Conv1D(32, 5, activation='relu'  , dilation_rate=1, strides=5, input_shape=(None, 16), padding=\"valid\"))\n",
    "model.add(MaxPooling1D(15, strides=15))\n",
    "model.add(Conv1D(64, 5 , activation='relu'  , dilation_rate=1, strides=5, input_shape=(None, 32), padding=\"valid\"))\n",
    "model.add(Conv1D(1, 5 , activation='linear', dilation_rate=1, strides=5, input_shape=(None, 64), padding=\"valid\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile and fit model\n",
    "model.compile(optimizer=adam(lr=0.0005), loss=\"mae\")\n",
    "\n",
    "history = model.fit_generator(train,\n",
    "                              steps_per_epoch=train_batches,\n",
    "                              epochs=epochs,\n",
    "                              verbose=2,\n",
    "                              callbacks=cb,\n",
    "                              validation_data=valid,\n",
    "                              validation_steps=valid_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "1a8c972d212557beb2f7550c02e62663f872a8ce"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-18364a379fea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Visualize accuracies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mperf_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwhat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# Visualize accuracies\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def perf_plot(history, what = 'loss'):\n",
    "    x = history.history[what]\n",
    "    val_x = history.history['val_' + what]\n",
    "    epochs = np.asarray(history.epoch) + 1\n",
    "\n",
    "    plt.plot(epochs, x, 'bo', label = \"Training \" + what)\n",
    "    plt.plot(epochs, val_x, 'b', label = \"Validation \" + what)\n",
    "    plt.title(\"Training and validation \" + what)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "perf_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_uuid": "3b6ad90328ba3962ede6d26ec5018c72f734f827"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18 seg_010eab 2624 [[[0.01918388]]]]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-d3a8333fb913>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mseg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/test/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mseg_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'acoustic_data'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_to_failure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id', dtype={\"time_to_failure\": np.float32})\n",
    "\n",
    "# Load each test data, create the feature matrix, get numeric prediction\n",
    "for i, seg_id in enumerate(submission.index):\n",
    "    seg = pd.read_csv('../input/test/' + seg_id + '.csv', dtype={'acoustic_data': np.float32})\n",
    "    x = create_features(seg)\n",
    "    predict = model.predict(np.expand_dims(x, 0))\n",
    "    print('\\r', i, seg_id, submission.shape[0], predict, end = '')\n",
    "    submission.time_to_failure[i] = predict\n",
    "\n",
    "submission.head()\n",
    "\n",
    "# Save\n",
    "submission.to_csv('submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
